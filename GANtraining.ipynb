{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importando o módulo .py com o Generator e Discriminator\n",
    "from gan import Discriminator, Generator  # Substitua 'seu_arquivo' pelo nome do arquivo .py\n",
    "# Carregando os dados\n",
    "df = pd.read_csv('data_gan.csv')\n",
    "target_variable_names=[\"Yield strength / MPa\",\n",
    "        \"Ultimate tensile strength / MPa\",\n",
    "        \"Elongation / %\",\n",
    "        \"Reduction of Area / %\",\n",
    "        \"Charpy temperature / °C\",\n",
    "        \"Charpy impact toughness / J\"\n",
    "        ,\"Weld ID\",\n",
    "        \"AC or DC\",\n",
    "        \"Electrode positive or negative\",\n",
    "        \"Type of weld\"]\n",
    "# Separando os dados\n",
    "# Vamos supor que 'Yield strength' é o alvo e as outras duas colunas são as entradas\n",
    "X = df.drop(columns=target_variable_names)\n",
    "\n",
    "# y vai ser a coluna alvo 'Yield strength / MPa'\n",
    "y = df['Yield strength / MPa']\n",
    "\n",
    "# Tratamento de NaNs (vamos ignorá-los neste caso para simular dados incompletos)\n",
    "X = X.fillna(0)  # Substitua por algum método apropriado se necessário\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Convertendo para tensores\n",
    "X_train = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 - Loss D: 2.026888370513916, Loss G: 0.7004261016845703\n",
      "Epoch 10/1000 - Loss D: 0.3482334315776825, Loss G: 0.6927841305732727\n",
      "Epoch 20/1000 - Loss D: 0.3492352366447449, Loss G: 0.6895255446434021\n",
      "Epoch 30/1000 - Loss D: 0.3501451015472412, Loss G: 0.6883739233016968\n",
      "Epoch 40/1000 - Loss D: 0.34801703691482544, Loss G: 0.6939952373504639\n",
      "Epoch 50/1000 - Loss D: 0.33919817209243774, Loss G: 0.7144559621810913\n",
      "Epoch 60/1000 - Loss D: 0.3235291838645935, Loss G: 0.7528449892997742\n",
      "Epoch 70/1000 - Loss D: 0.3064330816268921, Loss G: 0.7922957539558411\n",
      "Epoch 80/1000 - Loss D: 0.2908584177494049, Loss G: 0.833342969417572\n",
      "Epoch 90/1000 - Loss D: 0.2728399634361267, Loss G: 0.8828449845314026\n",
      "Epoch 100/1000 - Loss D: 0.2561086416244507, Loss G: 0.9312447905540466\n",
      "Epoch 110/1000 - Loss D: 0.2528555691242218, Loss G: 0.9403455853462219\n",
      "Epoch 120/1000 - Loss D: 0.2618933618068695, Loss G: 0.9137650728225708\n",
      "Epoch 130/1000 - Loss D: 0.2703411877155304, Loss G: 0.8941965699195862\n",
      "Epoch 140/1000 - Loss D: 0.25172147154808044, Loss G: 0.9525544047355652\n",
      "Epoch 150/1000 - Loss D: 0.2164587527513504, Loss G: 1.07712984085083\n",
      "Epoch 160/1000 - Loss D: 0.1812705099582672, Loss G: 1.2304491996765137\n",
      "Epoch 170/1000 - Loss D: 0.16056637465953827, Loss G: 1.332898497581482\n",
      "Epoch 180/1000 - Loss D: 0.15837107598781586, Loss G: 1.3395410776138306\n",
      "Epoch 190/1000 - Loss D: 0.1609918624162674, Loss G: 1.3221017122268677\n",
      "Epoch 200/1000 - Loss D: 0.16038772463798523, Loss G: 1.3254845142364502\n",
      "Epoch 210/1000 - Loss D: 0.14516539871692657, Loss G: 1.4139039516448975\n",
      "Epoch 220/1000 - Loss D: 0.12201737612485886, Loss G: 1.5645840167999268\n",
      "Epoch 230/1000 - Loss D: 0.11329580098390579, Loss G: 1.6250900030136108\n",
      "Epoch 240/1000 - Loss D: 0.10940852761268616, Loss G: 1.6513365507125854\n",
      "Epoch 250/1000 - Loss D: 0.10054738819599152, Loss G: 1.7267757654190063\n",
      "Epoch 260/1000 - Loss D: 0.08699806034564972, Loss G: 1.860453724861145\n",
      "Epoch 270/1000 - Loss D: 0.07114928215742111, Loss G: 2.0490212440490723\n",
      "Epoch 280/1000 - Loss D: 0.05758288502693176, Loss G: 2.2489614486694336\n",
      "Epoch 290/1000 - Loss D: 0.04730825871229172, Loss G: 2.43517804145813\n",
      "Epoch 300/1000 - Loss D: 0.03795897588133812, Loss G: 2.6476333141326904\n",
      "Epoch 310/1000 - Loss D: 0.029524344950914383, Loss G: 2.891356945037842\n",
      "Epoch 320/1000 - Loss D: 0.02346111834049225, Loss G: 3.11305570602417\n",
      "Epoch 330/1000 - Loss D: 0.019597753882408142, Loss G: 3.2854580879211426\n",
      "Epoch 340/1000 - Loss D: 0.01706191711127758, Loss G: 3.41965913772583\n",
      "Epoch 350/1000 - Loss D: 0.01500053983181715, Loss G: 3.5472140312194824\n",
      "Epoch 360/1000 - Loss D: 0.012908630073070526, Loss G: 3.6974575519561768\n",
      "Epoch 370/1000 - Loss D: 0.010848583653569221, Loss G: 3.8717262744903564\n",
      "Epoch 380/1000 - Loss D: 0.009035571478307247, Loss G: 4.055201053619385\n",
      "Epoch 390/1000 - Loss D: 0.0077712987549602985, Loss G: 4.208331108093262\n",
      "Epoch 400/1000 - Loss D: 0.007250543218106031, Loss G: 4.288804054260254\n",
      "Epoch 410/1000 - Loss D: 0.00808639731258154, Loss G: 4.220119476318359\n",
      "Epoch 420/1000 - Loss D: 0.01003052107989788, Loss G: 4.090437412261963\n",
      "Epoch 430/1000 - Loss D: 0.008396122604608536, Loss G: 4.320525169372559\n",
      "Epoch 440/1000 - Loss D: 0.007213551551103592, Loss G: 4.582995414733887\n",
      "Epoch 450/1000 - Loss D: 0.010980458930134773, Loss G: 4.473562240600586\n",
      "Epoch 460/1000 - Loss D: 0.00740720285102725, Loss G: 4.687673568725586\n",
      "Epoch 470/1000 - Loss D: 0.0073388428427278996, Loss G: 4.77683162689209\n",
      "Epoch 480/1000 - Loss D: 0.005771290045231581, Loss G: 5.023410797119141\n",
      "Epoch 490/1000 - Loss D: 0.0050161429680883884, Loss G: 5.344667434692383\n",
      "Epoch 500/1000 - Loss D: 0.0013648527674376965, Loss G: 6.315152645111084\n",
      "Epoch 510/1000 - Loss D: 0.0028198384679853916, Loss G: 5.519038200378418\n",
      "Epoch 520/1000 - Loss D: 0.002963359234854579, Loss G: 5.6254191398620605\n",
      "Epoch 530/1000 - Loss D: 0.0028412179090082645, Loss G: 5.686110973358154\n",
      "Epoch 540/1000 - Loss D: 0.003472954500466585, Loss G: 5.644131660461426\n",
      "Epoch 550/1000 - Loss D: 0.002738021546974778, Loss G: 5.961193084716797\n",
      "Epoch 560/1000 - Loss D: 0.0028895214200019836, Loss G: 5.98848819732666\n",
      "Epoch 570/1000 - Loss D: 0.0013599838130176067, Loss G: 6.667565822601318\n",
      "Epoch 580/1000 - Loss D: 0.0008633554098196328, Loss G: 6.837831497192383\n",
      "Epoch 590/1000 - Loss D: 0.002374146366491914, Loss G: 6.021828651428223\n",
      "Epoch 600/1000 - Loss D: 0.0017512929625809193, Loss G: 6.507508754730225\n",
      "Epoch 610/1000 - Loss D: 0.002187683479860425, Loss G: 6.357729911804199\n",
      "Epoch 620/1000 - Loss D: 0.002338989870622754, Loss G: 6.378635883331299\n",
      "Epoch 630/1000 - Loss D: 0.0012333495542407036, Loss G: 6.982563018798828\n",
      "Epoch 640/1000 - Loss D: 0.0014147469773888588, Loss G: 6.8439860343933105\n",
      "Epoch 650/1000 - Loss D: 0.0009787589078769088, Loss G: 7.332553863525391\n",
      "Epoch 660/1000 - Loss D: 0.0010992016177624464, Loss G: 7.358682155609131\n",
      "Epoch 670/1000 - Loss D: 0.0008818270289339125, Loss G: 7.347403049468994\n",
      "Epoch 680/1000 - Loss D: 0.0010571597376838326, Loss G: 6.98846435546875\n",
      "Epoch 690/1000 - Loss D: 0.0009621587232686579, Loss G: 7.1023077964782715\n",
      "Epoch 700/1000 - Loss D: 0.0012193643487989902, Loss G: 7.0291242599487305\n",
      "Epoch 710/1000 - Loss D: 0.0007730061188340187, Loss G: 7.305096626281738\n",
      "Epoch 720/1000 - Loss D: 0.0015809659380465746, Loss G: 6.727610111236572\n",
      "Epoch 730/1000 - Loss D: 0.0011458091903477907, Loss G: 7.023277759552002\n",
      "Epoch 740/1000 - Loss D: 0.0008163045276887715, Loss G: 7.449393272399902\n",
      "Epoch 750/1000 - Loss D: 0.0007347962236963212, Loss G: 7.403798580169678\n",
      "Epoch 760/1000 - Loss D: 0.0007277552504092455, Loss G: 7.245912075042725\n",
      "Epoch 770/1000 - Loss D: 0.00040273417835123837, Loss G: 7.784916400909424\n",
      "Epoch 780/1000 - Loss D: 0.0006502586184069514, Loss G: 7.129886150360107\n",
      "Epoch 790/1000 - Loss D: 0.0008708576206117868, Loss G: 7.163366317749023\n",
      "Epoch 800/1000 - Loss D: 0.0006563553470186889, Loss G: 7.498913288116455\n",
      "Epoch 810/1000 - Loss D: 0.0009998091263696551, Loss G: 7.298953533172607\n",
      "Epoch 820/1000 - Loss D: 0.0007171297329477966, Loss G: 7.633737087249756\n",
      "Epoch 830/1000 - Loss D: 0.0005215160199441016, Loss G: 7.88207483291626\n",
      "Epoch 840/1000 - Loss D: 0.0006000453722663224, Loss G: 7.754551410675049\n",
      "Epoch 850/1000 - Loss D: 0.0004258831322658807, Loss G: 8.140722274780273\n",
      "Epoch 860/1000 - Loss D: 0.00024052739900071174, Loss G: 8.448339462280273\n",
      "Epoch 870/1000 - Loss D: 0.00032865331741049886, Loss G: 8.049339294433594\n",
      "Epoch 880/1000 - Loss D: 0.0003063757612835616, Loss G: 8.25031852722168\n",
      "Epoch 890/1000 - Loss D: 0.0003311758046038449, Loss G: 8.132283210754395\n",
      "Epoch 900/1000 - Loss D: 0.000406209088396281, Loss G: 8.135805130004883\n",
      "Epoch 910/1000 - Loss D: 0.00040335505036637187, Loss G: 8.15084457397461\n",
      "Epoch 920/1000 - Loss D: 0.0003875134279951453, Loss G: 8.194771766662598\n",
      "Epoch 930/1000 - Loss D: 0.0005386389093473554, Loss G: 8.164055824279785\n",
      "Epoch 940/1000 - Loss D: 0.0002964729501400143, Loss G: 8.471996307373047\n",
      "Epoch 950/1000 - Loss D: 0.00034709888859651983, Loss G: 8.228123664855957\n",
      "Epoch 960/1000 - Loss D: 0.00034177201450802386, Loss G: 8.24006462097168\n",
      "Epoch 970/1000 - Loss D: 0.0003037441929336637, Loss G: 8.347306251525879\n",
      "Epoch 980/1000 - Loss D: 0.00030188215896487236, Loss G: 8.286664962768555\n",
      "Epoch 990/1000 - Loss D: 0.0004377407894935459, Loss G: 8.20218276977539\n",
      "generated data:  tensor([[-2.1252, -2.0549, -3.5410,  ...,  7.0676,  3.4872,  1.8276],\n",
      "        [-2.2365, -2.1693, -3.9826,  ...,  7.8056,  3.8917,  2.0128],\n",
      "        [-2.7863, -2.9416, -5.0964,  ...,  9.2359,  4.7566,  2.5032],\n",
      "        ...,\n",
      "        [-1.9505, -2.1665, -3.7977,  ...,  7.0434,  3.5312,  1.8004],\n",
      "        [-2.2962, -2.3649, -4.1546,  ...,  8.0722,  3.9570,  2.0678],\n",
      "        [-3.0052, -2.7474, -5.0355,  ...,  9.3089,  4.7502,  2.2789]])\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o generator e discriminator\n",
    "generator = Generator(size = X_train.shape[1])  # Tamanho baseado no número de features\n",
    "discriminator = Discriminator(size = X_train.shape[1])\n",
    "\n",
    "# Definindo otimizadores\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Função de perda\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Função de treinamento da GAN\n",
    "def train_GAN(epochs=10000, print_every=1000):\n",
    "    for epoch in range(epochs):\n",
    "        # Gerar ruído\n",
    "        z = torch.randn(X_train.size(0), X_train.size(1))  # Ruído no formato das features\n",
    "        \n",
    "        # Gerar amostras falsas (saídas do generator)\n",
    "        fake_data = generator(z)\n",
    "        \n",
    "        # Treinando o Discriminator\n",
    "        real_preds = discriminator(X_train)\n",
    "        fake_preds = discriminator(fake_data.detach())\n",
    "        \n",
    "        # Definindo rótulos\n",
    "        real_labels = torch.ones(X_train.size(0), 1)\n",
    "        fake_labels = torch.zeros(X_train.size(0), 1)\n",
    "        \n",
    "        # Calculando a perda do Discriminator\n",
    "        loss_real = criterion(real_preds, real_labels)\n",
    "        loss_fake = criterion(fake_preds, fake_labels)\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        \n",
    "        # Otimizando o Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Treinando o Generator\n",
    "        fake_preds = discriminator(fake_data)\n",
    "        loss_G = criterion(fake_preds, real_labels)  # O generator quer enganar o discriminator\n",
    "        \n",
    "        # Otimizando o Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Exibir o progresso\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch}/{epochs} - Loss D: {loss_D.item()}, Loss G: {loss_G.item()}')\n",
    "\n",
    "# Treinar a GAN\n",
    "train_GAN(epochs=1000, print_every=10)\n",
    "\n",
    "# Gerando novas amostras\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(100, X_train.size(1))\n",
    "    generated_data = generator(z)\n",
    "    print(\"generated data: \",generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300 - Loss D: 3.00421404838562, Loss G: 0.770463228225708\n",
      "Epoch 10/300 - Loss D: 3.117750644683838, Loss G: 0.7702500820159912\n",
      "Epoch 20/300 - Loss D: 3.03877329826355, Loss G: 0.770110547542572\n",
      "Epoch 30/300 - Loss D: 3.5929601192474365, Loss G: 0.7704694867134094\n",
      "Epoch 40/300 - Loss D: 3.6486451625823975, Loss G: 0.7706210613250732\n",
      "Epoch 50/300 - Loss D: 3.0697877407073975, Loss G: 0.7704582214355469\n",
      "Epoch 60/300 - Loss D: 3.9120967388153076, Loss G: 0.7699591517448425\n",
      "Epoch 70/300 - Loss D: 2.9404056072235107, Loss G: 0.7705283761024475\n",
      "Epoch 80/300 - Loss D: 3.3941307067871094, Loss G: 0.7703944444656372\n",
      "Epoch 90/300 - Loss D: 3.3264594078063965, Loss G: 0.7703354954719543\n",
      "Epoch 100/300 - Loss D: 3.4735705852508545, Loss G: 0.770569920539856\n",
      "Epoch 110/300 - Loss D: 2.918048858642578, Loss G: 0.7705375552177429\n",
      "Epoch 120/300 - Loss D: 2.814788341522217, Loss G: 0.7708379030227661\n",
      "Epoch 130/300 - Loss D: 2.910727024078369, Loss G: 0.7705462574958801\n",
      "Epoch 140/300 - Loss D: 3.3952417373657227, Loss G: 0.7702764272689819\n",
      "Epoch 150/300 - Loss D: 2.939021348953247, Loss G: 0.7703016996383667\n",
      "Epoch 160/300 - Loss D: 3.475978136062622, Loss G: 0.7706301808357239\n",
      "Epoch 170/300 - Loss D: 3.654222011566162, Loss G: 0.7705268263816833\n",
      "Epoch 180/300 - Loss D: 2.990952253341675, Loss G: 0.769887387752533\n",
      "Epoch 190/300 - Loss D: 3.171903371810913, Loss G: 0.7707630395889282\n",
      "Epoch 200/300 - Loss D: 2.988682746887207, Loss G: 0.770358145236969\n",
      "Epoch 210/300 - Loss D: 3.1578192710876465, Loss G: 0.7702747583389282\n",
      "Epoch 220/300 - Loss D: 3.0941858291625977, Loss G: 0.7699490785598755\n",
      "Epoch 230/300 - Loss D: 3.2517855167388916, Loss G: 0.7701313495635986\n",
      "Epoch 240/300 - Loss D: 3.0452253818511963, Loss G: 0.7705350518226624\n",
      "Epoch 250/300 - Loss D: 3.496995687484741, Loss G: 0.7703644633293152\n",
      "Epoch 260/300 - Loss D: 3.114234685897827, Loss G: 0.7703436613082886\n",
      "Epoch 270/300 - Loss D: 3.4356319904327393, Loss G: 0.7704561948776245\n",
      "Epoch 280/300 - Loss D: 3.12217378616333, Loss G: 0.7703128457069397\n",
      "Epoch 290/300 - Loss D: 3.4132018089294434, Loss G: 0.7704960703849792\n",
      "tensor([[0.4535],\n",
      "        [0.4343],\n",
      "        [0.4520],\n",
      "        [0.4531],\n",
      "        [0.4568],\n",
      "        [0.4566],\n",
      "        [0.4434],\n",
      "        [0.4514],\n",
      "        [0.4580],\n",
      "        [0.4545],\n",
      "        [0.4507],\n",
      "        [0.4608],\n",
      "        [0.4567],\n",
      "        [0.4603],\n",
      "        [0.4619],\n",
      "        [0.4657],\n",
      "        [0.4553],\n",
      "        [0.4420],\n",
      "        [0.4581],\n",
      "        [0.4493],\n",
      "        [0.4415],\n",
      "        [0.4389],\n",
      "        [0.4564],\n",
      "        [0.4549],\n",
      "        [0.4608],\n",
      "        [0.4603],\n",
      "        [0.4432],\n",
      "        [0.4623],\n",
      "        [0.4548],\n",
      "        [0.4480],\n",
      "        [0.4512],\n",
      "        [0.4552],\n",
      "        [0.4452],\n",
      "        [0.4542],\n",
      "        [0.4561],\n",
      "        [0.4409],\n",
      "        [0.4599],\n",
      "        [0.4582],\n",
      "        [0.4512],\n",
      "        [0.4592],\n",
      "        [0.4529],\n",
      "        [0.4538],\n",
      "        [0.4335],\n",
      "        [0.4414],\n",
      "        [0.4564],\n",
      "        [0.4618],\n",
      "        [0.4361],\n",
      "        [0.4483],\n",
      "        [0.4472],\n",
      "        [0.4467],\n",
      "        [0.4408],\n",
      "        [0.4590],\n",
      "        [0.4544],\n",
      "        [0.4543],\n",
      "        [0.4565],\n",
      "        [0.4682],\n",
      "        [0.4542],\n",
      "        [0.4625],\n",
      "        [0.4475],\n",
      "        [0.4451],\n",
      "        [0.4536],\n",
      "        [0.4618],\n",
      "        [0.4495],\n",
      "        [0.4570],\n",
      "        [0.4543],\n",
      "        [0.4621],\n",
      "        [0.4573],\n",
      "        [0.4458],\n",
      "        [0.4619],\n",
      "        [0.4465],\n",
      "        [0.4495],\n",
      "        [0.4439],\n",
      "        [0.4414],\n",
      "        [0.4627],\n",
      "        [0.4455],\n",
      "        [0.4515],\n",
      "        [0.4373],\n",
      "        [0.4515],\n",
      "        [0.4438],\n",
      "        [0.4615],\n",
      "        [0.4478],\n",
      "        [0.4420],\n",
      "        [0.4495],\n",
      "        [0.4484],\n",
      "        [0.4553],\n",
      "        [0.4591],\n",
      "        [0.4630],\n",
      "        [0.4402],\n",
      "        [0.4409],\n",
      "        [0.4577],\n",
      "        [0.4464],\n",
      "        [0.4636],\n",
      "        [0.4591],\n",
      "        [0.4522],\n",
      "        [0.4541],\n",
      "        [0.4588],\n",
      "        [0.4512],\n",
      "        [0.4388],\n",
      "        [0.4537],\n",
      "        [0.4592],\n",
      "        [0.4582],\n",
      "        [0.4385],\n",
      "        [0.4506],\n",
      "        [0.4413],\n",
      "        [0.4465],\n",
      "        [0.4577],\n",
      "        [0.4541],\n",
      "        [0.4448],\n",
      "        [0.4599],\n",
      "        [0.4372],\n",
      "        [0.4513],\n",
      "        [0.4533],\n",
      "        [0.4576],\n",
      "        [0.4582],\n",
      "        [0.4395],\n",
      "        [0.4297],\n",
      "        [0.4505],\n",
      "        [0.4478],\n",
      "        [0.4500],\n",
      "        [0.4446],\n",
      "        [0.4386],\n",
      "        [0.4531],\n",
      "        [0.4547],\n",
      "        [0.4580],\n",
      "        [0.4531],\n",
      "        [0.4470],\n",
      "        [0.4557],\n",
      "        [0.4587],\n",
      "        [0.4497],\n",
      "        [0.4504],\n",
      "        [0.4492],\n",
      "        [0.4639],\n",
      "        [0.4457],\n",
      "        [0.4276],\n",
      "        [0.4550],\n",
      "        [0.4550],\n",
      "        [0.4452],\n",
      "        [0.4353],\n",
      "        [0.4572],\n",
      "        [0.4599],\n",
      "        [0.4529],\n",
      "        [0.4503],\n",
      "        [0.4366],\n",
      "        [0.4535],\n",
      "        [0.4490],\n",
      "        [0.4497],\n",
      "        [0.4528],\n",
      "        [0.4558],\n",
      "        [0.4435],\n",
      "        [0.4486],\n",
      "        [0.4471],\n",
      "        [0.4510],\n",
      "        [0.4552],\n",
      "        [0.4549],\n",
      "        [0.4578],\n",
      "        [0.4473],\n",
      "        [0.4445],\n",
      "        [0.4493],\n",
      "        [0.4461],\n",
      "        [0.4465],\n",
      "        [0.4519],\n",
      "        [0.4530],\n",
      "        [0.4549],\n",
      "        [0.4503],\n",
      "        [0.4431],\n",
      "        [0.4476],\n",
      "        [0.4448],\n",
      "        [0.4470],\n",
      "        [0.4621],\n",
      "        [0.4423],\n",
      "        [0.4485],\n",
      "        [0.4503],\n",
      "        [0.4522],\n",
      "        [0.4576],\n",
      "        [0.4416],\n",
      "        [0.4478],\n",
      "        [0.4565],\n",
      "        [0.4519],\n",
      "        [0.4563],\n",
      "        [0.4356],\n",
      "        [0.4478],\n",
      "        [0.4346],\n",
      "        [0.4617],\n",
      "        [0.4364],\n",
      "        [0.4586],\n",
      "        [0.4428],\n",
      "        [0.4268],\n",
      "        [0.4488],\n",
      "        [0.4550],\n",
      "        [0.4435],\n",
      "        [0.4447],\n",
      "        [0.4600],\n",
      "        [0.4608],\n",
      "        [0.4600],\n",
      "        [0.4495],\n",
      "        [0.4429],\n",
      "        [0.4522],\n",
      "        [0.4376],\n",
      "        [0.4453],\n",
      "        [0.4486],\n",
      "        [0.4536],\n",
      "        [0.4474],\n",
      "        [0.4494],\n",
      "        [0.4329],\n",
      "        [0.4395],\n",
      "        [0.4493],\n",
      "        [0.4493],\n",
      "        [0.4489],\n",
      "        [0.4534],\n",
      "        [0.4538],\n",
      "        [0.4655],\n",
      "        [0.4480],\n",
      "        [0.4512],\n",
      "        [0.4473],\n",
      "        [0.4662],\n",
      "        [0.4516],\n",
      "        [0.4411],\n",
      "        [0.4502],\n",
      "        [0.4579],\n",
      "        [0.4630],\n",
      "        [0.4556],\n",
      "        [0.4510],\n",
      "        [0.4571],\n",
      "        [0.4421],\n",
      "        [0.4451],\n",
      "        [0.4494],\n",
      "        [0.4579],\n",
      "        [0.4493],\n",
      "        [0.4488],\n",
      "        [0.4429],\n",
      "        [0.4643],\n",
      "        [0.4334],\n",
      "        [0.4512],\n",
      "        [0.4510],\n",
      "        [0.4588],\n",
      "        [0.4546],\n",
      "        [0.4529],\n",
      "        [0.4508],\n",
      "        [0.4588],\n",
      "        [0.4509],\n",
      "        [0.4559],\n",
      "        [0.4434],\n",
      "        [0.4410],\n",
      "        [0.4448],\n",
      "        [0.4576],\n",
      "        [0.4597],\n",
      "        [0.4493],\n",
      "        [0.4373],\n",
      "        [0.4580],\n",
      "        [0.4547],\n",
      "        [0.4556],\n",
      "        [0.4638],\n",
      "        [0.4635],\n",
      "        [0.4490],\n",
      "        [0.4525],\n",
      "        [0.4553],\n",
      "        [0.4550],\n",
      "        [0.4477],\n",
      "        [0.4423],\n",
      "        [0.4536],\n",
      "        [0.4431],\n",
      "        [0.4511],\n",
      "        [0.4481],\n",
      "        [0.4630],\n",
      "        [0.4583],\n",
      "        [0.4542],\n",
      "        [0.4404],\n",
      "        [0.4567],\n",
      "        [0.4512],\n",
      "        [0.4510],\n",
      "        [0.4430],\n",
      "        [0.4505],\n",
      "        [0.4549],\n",
      "        [0.4426],\n",
      "        [0.4560],\n",
      "        [0.4467],\n",
      "        [0.4312],\n",
      "        [0.4611],\n",
      "        [0.4536],\n",
      "        [0.4406],\n",
      "        [0.4552],\n",
      "        [0.4589],\n",
      "        [0.4468],\n",
      "        [0.4328],\n",
      "        [0.4562],\n",
      "        [0.4566],\n",
      "        [0.4570],\n",
      "        [0.4514],\n",
      "        [0.4448],\n",
      "        [0.4540],\n",
      "        [0.4565],\n",
      "        [0.4585],\n",
      "        [0.4467],\n",
      "        [0.4405],\n",
      "        [0.4544],\n",
      "        [0.4636],\n",
      "        [0.4501],\n",
      "        [0.4670],\n",
      "        [0.4564],\n",
      "        [0.4476],\n",
      "        [0.4328],\n",
      "        [0.4594],\n",
      "        [0.4605],\n",
      "        [0.4572],\n",
      "        [0.4590],\n",
      "        [0.4523],\n",
      "        [0.4563],\n",
      "        [0.4471],\n",
      "        [0.4673],\n",
      "        [0.4460],\n",
      "        [0.4632],\n",
      "        [0.4582],\n",
      "        [0.4563],\n",
      "        [0.4533],\n",
      "        [0.4548],\n",
      "        [0.4569],\n",
      "        [0.4393],\n",
      "        [0.4356],\n",
      "        [0.4523],\n",
      "        [0.4397],\n",
      "        [0.4477],\n",
      "        [0.4505],\n",
      "        [0.4353],\n",
      "        [0.4529],\n",
      "        [0.4420],\n",
      "        [0.4546],\n",
      "        [0.4552],\n",
      "        [0.4495],\n",
      "        [0.4419],\n",
      "        [0.4546],\n",
      "        [0.4575]])\n",
      "Shape of y_true: (331, 1)\n",
      "Shape of y_pred: (331, 1)\n",
      "MSE: 137101.70364857663, MAE: 258.3646081543221, R²: -0.9424961466005766\n"
     ]
    }
   ],
   "source": [
    "# Dividindo o dataset em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "generator = Generator(size = X_train.shape[1])  # Tamanho baseado no número de features\n",
    "discriminator = Discriminator(size = X_train.shape[1])\n",
    "\n",
    "# X são as features e y é a coluna alvo 'Yield strength'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Convertendo os dados em tensores\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Função de treinamento da GAN (mesma de antes)\n",
    "def train_GAN(epochs=10000, print_every=1000):\n",
    "    for epoch in range(epochs):\n",
    "        for real_data, _ in train_loader:  # Itera sobre batches\n",
    "            # Gerar ruído\n",
    "            z = torch.randn(real_data.size(0), real_data.size(1))  # Ruído no formato das features\n",
    "            \n",
    "            # Gerar amostras falsas (saídas do generator)\n",
    "            fake_data = generator(z)\n",
    "            \n",
    "            # Treinando o Discriminator\n",
    "            real_preds = discriminator(real_data)\n",
    "            fake_preds = discriminator(fake_data.detach())\n",
    "            \n",
    "            # Definindo rótulos\n",
    "            real_labels = torch.ones(real_data.size(0), 1)\n",
    "            fake_labels = torch.zeros(real_data.size(0), 1)\n",
    "            \n",
    "            # Calculando a perda do Discriminator\n",
    "            loss_real = criterion(real_preds, real_labels)\n",
    "            loss_fake = criterion(fake_preds, fake_labels)\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "            \n",
    "            # Otimizando o Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Treinando o Generator\n",
    "            fake_preds = discriminator(fake_data)\n",
    "            loss_G = criterion(fake_preds, real_labels)  # O generator quer enganar o discriminator\n",
    "            \n",
    "            # Otimizando o Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        # Exibir o progresso\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch}/{epochs} - Loss D: {loss_D.item()}, Loss G: {loss_G.item()}')\n",
    "\n",
    "# Treinar a GAN\n",
    "train_GAN(epochs=300, print_every=10)\n",
    "\n",
    "# Avaliação do Generator após o treino\n",
    "with torch.no_grad():\n",
    "    z_test = torch.randn(X_test_tensor.size(0), X_test_tensor.size(1))  # Gerar ruído para o conjunto de teste\n",
    "    generated_data = discriminator(z_test) #ultuma linha é o y\n",
    "\n",
    "# Comparar os resultados gerados com o conjunto de teste real\n",
    "print(generated_data)\n",
    "y_pred = generated_data.numpy()\n",
    "y_true = y_test_tensor.numpy()\n",
    "print(f'Shape of y_true: {y_true.shape}')\n",
    "print(f'Shape of y_pred: {y_pred.shape}')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}, MAE: {mae}, R²: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Yield Strength Predictions\n",
      "0                      0.464033\n",
      "1                      0.460200\n",
      "2                      0.459471\n",
      "3                      0.456909\n",
      "4                      0.451099\n",
      "..                          ...\n",
      "326                    0.450998\n",
      "327                    0.440055\n",
      "328                    0.458349\n",
      "329                    0.458706\n",
      "330                    0.467054\n",
      "\n",
      "[331 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_test = torch.randn(X_test_tensor.size(0), X_test_tensor.size(1))  # Gerar ruído para o conjunto de teste\n",
    "    generated_data = discriminator(z_test)  # Aqui, você gera dados com 35 colunas\n",
    "\n",
    "# Selecionando apenas a coluna 'Yield strength', que é a última coluna\n",
    "yield_strength_predictions = generated_data[:, -1]  # Seleciona a última coluna\n",
    "\n",
    "# Convertendo para numpy e criando um DataFrame para visualização\n",
    "yield_strength_predictions_np = yield_strength_predictions.numpy()\n",
    "df_predictions = pd.DataFrame(yield_strength_predictions_np, columns=['Yield Strength Predictions'])\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
